{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a62e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1cdaf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = \"./pathpiece/data/vdump_32768.vocab\"\n",
    "# this is hardcoded in the tokenizer\n",
    "eos_text = \"<|endoftext|>\"  # default\n",
    "greedy = False  # default\n",
    "random_tiebreaker = True # old value\n",
    "\n",
    "tokenizer = pathpiece.Tokenizer(vocab_file, random_tiebreaker=random_tiebreaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03045b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is added as token zero automatically \n",
    "# that is, it isn't present in the .vocab file\n",
    "# Note it isn't automatically used:\n",
    "# It is up to the caller to add it at the end of the document\n",
    "tokenizer.encode(eos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1efc3ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can also pass in some other end-of-document token\n",
    "tokenizer2 = pathpiece.Tokenizer(vocab_file, special=\"<eos>\")\n",
    "# now this is token 0\n",
    "tokenizer2.encode(\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d4e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"The quick brown fox\"\n",
    "en = tokenizer.encode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc384bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[14004, 10800, 31267, 8561, 7389],\n",
       "  [16259, 15656, 3800, 1306, 24019, 16452, 3865, 24659]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given a list of strings, \n",
    "# this tokenizes each document in parallel\n",
    "# returns a list of token id lists\n",
    "parallel = tokenizer.encode_batch([\"The quick brown fox\", \"jumped over the lazy dog.\"])\n",
    "parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b91720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox', 'jumped over the lazy dog.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is also a parallel decode \n",
    "# that takes a list of list of token ids\n",
    "tokenizer.decode_batch(parallel[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e5834be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [14004, 10800, 31267, 8561, 7389]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matching the huggingface interface, \n",
    "# you can also __call__ the tokenizer to encode\n",
    "# if you pass a list of strings, this uses encode_batch\n",
    "# any extra parameters (often passed to huggingface tokenizer) will be ignored\n",
    "tokenizer(\"The quick brown fox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d4ce8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conver back again with decode\n",
    "back = tokenizer.decode(en[\"input_ids\"])\n",
    "assert back == s\n",
    "back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49202c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the maximum number of bytes in any token\n",
    "tokenizer.get_max_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd8df189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'6d', 14838),\n",
       " (b' OR', 254),\n",
       " (b'risto', 373),\n",
       " (b'. We are', 27207),\n",
       " (b' (see', 8459),\n",
       " (b's. To', 20120),\n",
       " (b'lush', 6195),\n",
       " (b'Na', 26799),\n",
       " (b'LD', 22995),\n",
       " (b'509', 30546)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the token : id mapping as dict\n",
    "# note it comes in random order each time the vocab is read\n",
    "vocab = tokenizer.get_vocab()\n",
    "list(vocab.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e603ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32769"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the vocabulary size do\n",
    "# remember that one of these is for the <|endoftext|> special token\n",
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e8a41a6-5d59-45d7-a808-d672012df853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get the unjoined token lists like this\n",
    "ids = tokenizer.get_ids()\n",
    "def get_token_list(enc):\n",
    "    return [ids[i] for i in enc[\"input_ids\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2854f37a-9e63-4870-a2f6-d1f8c9d47ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'The ',\n",
       " b'quick',\n",
       " b' br',\n",
       " b'own ',\n",
       " b'fox',\n",
       " b' jump',\n",
       " b'ed over',\n",
       " b' the l',\n",
       " b'az',\n",
       " b'y ',\n",
       " b'do',\n",
       " b'g.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = get_token_list(tokenizer(\"The quick brown fox jumped over the lazy dog.\"))\n",
    "print(len(opt))\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7007803-8387-41bb-8419-1aeae03c9c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'The ',\n",
       " b'quick',\n",
       " b' brown',\n",
       " b' fo',\n",
       " b'x ',\n",
       " b'ju',\n",
       " b'mp',\n",
       " b'ed over',\n",
       " b' the l',\n",
       " b'az',\n",
       " b'y d',\n",
       " b'og',\n",
       " b'.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the greedy version\n",
    "gr = pathpiece.Tokenizer(vocab_file, greedy=True)\n",
    "tl = get_token_list(gr(\"The quick brown fox jumped over the lazy dog.\"))\n",
    "print(len(tl))\n",
    "tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b832b79e-4068-40f8-9bd8-79effb1f1d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'The',\n",
       " b' quick',\n",
       " b' br',\n",
       " b'own ',\n",
       " b'fox',\n",
       " b' jump',\n",
       " b'ed',\n",
       " b' over the',\n",
       " b' la',\n",
       " b'zy',\n",
       " b' do',\n",
       " b'g.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the longest tiebreaker version\n",
    "long = pathpiece.Tokenizer(vocab_file, random_tiebreaker=False)\n",
    "tl = get_token_list(long(\"The quick brown fox jumped over the lazy dog.\"))\n",
    "print(len(tl))\n",
    "tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d34227b-b284-47b2-b65c-18fbbf306ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14004, 10800, 31267, 8561, 7389, 31747, 3800, 1306, 24019, 23329, 30199, 8916]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = tokenizer.encode(\"The quick brown fox jumped over the lazy dog.\")['input_ids']\n",
    "print(len(opt))\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df4b7f52-7c69-4481-92c6-3fe6c586cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also pass in some other end-of-document token\n",
    "# verify the greedy params work\n",
    "# specify this for __init__, so don't need to modify calling encoding code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f26b6056-9fce-452e-8fd0-6e99a5c991ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "[14004, 10800, 12759, 6623, 22140, 16259, 15656, 3800, 1306, 24019, 16452, 3865, 24659]\n"
     ]
    }
   ],
   "source": [
    "tokenizer3 = pathpiece.Tokenizer(vocab_file, special=eos_text, greedy=True)\n",
    "\n",
    "gr3 = tokenizer3.encode(\"The quick brown fox jumped over the lazy dog.\")['input_ids']\n",
    "print(len(gr3))\n",
    "print(gr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dfa4d77-7930-4040-8f28-8a3f64748d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "[14004, 10800, 12759, 6623, 22140, 16259, 15656, 3800, 1306, 24019, 16452, 3865, 24659]\n"
     ]
    }
   ],
   "source": [
    "tokenizer4 = pathpiece.Tokenizer(vocab_file, eos_text, True)  # greedy positional\n",
    "\n",
    "gr4 = tokenizer4.encode(\"The quick brown fox jumped over the lazy dog.\")['input_ids']\n",
    "print(len(gr4))\n",
    "print(gr4)\n",
    "assert gr3 == gr4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18ff7a0c-384f-489e-b875-7001367656c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "[14004, 10800, 12759, 6623, 22140, 16259, 15656, 3800, 1306, 24019, 16452, 3865, 24659]\n"
     ]
    }
   ],
   "source": [
    "tokenizer5 = pathpiece.Tokenizer(vocab_file, greedy=True)     # keyword without special\n",
    "\n",
    "gr5 = tokenizer5.encode(\"The quick brown fox jumped over the lazy dog.\")['input_ids']\n",
    "print(len(gr5))\n",
    "print(gr5)\n",
    "assert gr3 == gr5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59498003-dee7-4691-9fb8-ace4a28edcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'The',\n",
       " b' quick',\n",
       " b' br',\n",
       " b'own ',\n",
       " b'fox',\n",
       " b' jump',\n",
       " b'ed',\n",
       " b' over the',\n",
       " b' la',\n",
       " b'zy',\n",
       " b' do',\n",
       " b'g.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the longest tiebreaker version with all params\n",
    "long = pathpiece.Tokenizer(vocab_file, special=eos_text, greedy=False, random_tiebreaker=False)\n",
    "tl = get_token_list(long(\"The quick brown fox jumped over the lazy dog.\"))\n",
    "print(len(tl))\n",
    "tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcaf886-c8cf-4d0e-bba9-fba152eb9683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
